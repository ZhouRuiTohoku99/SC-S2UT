{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usrs/zhou/.pyenv/versions/anaconda3-2020.02/envs/zhou_conda/lib/python3.10/site-packages/df/io.py:9: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pathlib as pl\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import tqdm\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler, IterableDataset\n",
    "from speechbrain.dataio.dataloader import LoopedLoader, SaveableDataLoader\n",
    "import speechbrain as sb\n",
    "from speechbrain.inference.ASR import EncoderDecoderASR\n",
    "from speechbrain.inference.vocoders import UnitHIFIGAN,HIFIGAN\n",
    "from enum import Enum, auto\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage(Enum):\n",
    "    \"\"\"Simple enum to track stage of experiments.\"\"\"\n",
    "\n",
    "    TRAIN = auto()\n",
    "    VALID = auto()\n",
    "    TEST = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataio_prepare(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\n",
    "    \"\"\"\n",
    "    codes_folder = pl.Path(hparams[\"codes_folder\"])\n",
    "\n",
    "    # Define audio pipeline. In this case, we simply read the audio contained\n",
    "    # in the variable src_audio with the custom reader.\n",
    "    @sb.utils.data_pipeline.takes(\"src_audio\")\n",
    "    @sb.utils.data_pipeline.provides(\"src_sig\",\"src_mel\")\n",
    "    def src_audio_pipeline(wav):\n",
    "        \"\"\"Load the source language audio signal.\n",
    "        This is done on the CPU in the `collate_fn`\n",
    "        \"\"\"\n",
    "        info = torchaudio.info(wav)\n",
    "        sig = sb.dataio.dataio.read_audio(wav)\n",
    "        sig = torchaudio.transforms.Resample(\n",
    "            info.sample_rate, hparams[\"sample_rate\"]\n",
    "        )(sig)\n",
    "        mel_spec = hparams[\"mel_spectogram\"](audio=sig).transpose(0,1)\n",
    "        return sig, mel_spec\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"tgt_audio\")\n",
    "    @sb.utils.data_pipeline.provides(\"tgt_sig\")\n",
    "    def tgt_audio_pipeline(wav):\n",
    "        \"\"\"Load the target language audio signal.\n",
    "        This is done on the CPU in the `collate_fn`.\n",
    "        \"\"\"\n",
    "        info = torchaudio.info(wav)\n",
    "        sig = sb.dataio.dataio.read_audio(wav)\n",
    "        sig = torchaudio.transforms.Resample(\n",
    "            info.sample_rate,\n",
    "            hparams[\"sample_rate\"],\n",
    "        )(sig)\n",
    "        return sig\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"id\")\n",
    "    @sb.utils.data_pipeline.provides(\"code_bos\", \"code_eos\")\n",
    "    def unit_pipeline(utt_id):\n",
    "        \"\"\"Load target codes\"\"\"\n",
    "        code = np.load(codes_folder / f\"{utt_id}_tgt.npy\")\n",
    "        code = torch.LongTensor(code)\n",
    "        code = torch.unique_consecutive(code)\n",
    "        code_bos = torch.cat((torch.LongTensor([hparams[\"bos_index\"]]), code))\n",
    "        yield code_bos\n",
    "        code_eos = torch.cat((code, torch.LongTensor([hparams[\"eos_index\"]])))\n",
    "        yield code_eos\n",
    "\n",
    "    datasets = {}\n",
    "    for split in hparams[\"splits\"]:\n",
    "        datasets[split] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "            json_path=hparams[f\"{split}_json\"],\n",
    "            dynamic_items=[\n",
    "                src_audio_pipeline,\n",
    "                tgt_audio_pipeline,\n",
    "                unit_pipeline,\n",
    "            ],\n",
    "            output_keys=[\n",
    "                \"id\",\n",
    "                \"src_sig\",\n",
    "                \"src_mel\",\n",
    "                \"tgt_sig\",\n",
    "                \"duration\",\n",
    "                \"code_bos\",\n",
    "                \"code_eos\",\n",
    "                \"tgt_text\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    # Sorting training data with ascending order makes the code  much\n",
    "    # faster  because we minimize zero-padding. In most of the cases, this\n",
    "    # does not harm the performance.\n",
    "    if hparams[\"sorting\"] == \"ascending\":\n",
    "        datasets[\"train\"] = datasets[\"train\"].filtered_sorted(\n",
    "            sort_key=\"duration\"\n",
    "        )\n",
    "        datasets[\"valid\"] = datasets[\"valid\"].filtered_sorted(\n",
    "            sort_key=\"duration\"\n",
    "        )\n",
    "\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "        hparams[\"valid_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"descending\":\n",
    "        datasets[\"train\"] = datasets[\"train\"].filtered_sorted(\n",
    "            sort_key=\"duration\", reverse=True\n",
    "        )\n",
    "        datasets[\"valid\"] = datasets[\"valid\"].filtered_sorted(\n",
    "            sort_key=\"duration\", reverse=True\n",
    "        )\n",
    "\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "        hparams[\"valid_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"random\":\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = True\n",
    "        hparams[\"valid_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"sorting must be random, ascending or descending\"\n",
    "        )\n",
    "\n",
    "    # Dynamic Batching is used, we instantiate the needed samplers.\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class S2UT(sb.core.Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Computes the forward pass.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch : torch.Tensor or tensors\n",
    "            An element from the dataloader, including inputs for processing.\n",
    "        stage : Stage\n",
    "            The stage of the experiment: Stage.TRAIN, Stage.VALID, Stage.TEST\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (torch.Tensor or torch.Tensors, list of float or None, list of str or None)\n",
    "            The outputs after all processing is complete.\n",
    "        \"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.src_sig\n",
    "        tokens_bos, _ = batch.code_bos\n",
    "\n",
    "        # Use default padding value for wav2vec2\n",
    "        wavs[wavs == self.hparams.pad_index] = 0.0\n",
    "        # compute features\n",
    "        enc_out = self.modules.wav2vec2(wavs, wav_lens)\n",
    "\n",
    "        # dimensionality reduction\n",
    "        enc_out = self.modules.enc(enc_out)\n",
    "\n",
    "        if isinstance(self.modules.transformer, DistributedDataParallel):\n",
    "            dec_out = self.modules.transformer.module.forward_mt_decoder_only(\n",
    "                enc_out, tokens_bos, pad_idx=self.hparams.pad_index\n",
    "            )\n",
    "        else:\n",
    "            dec_out = self.modules.transformer.forward_mt_decoder_only(\n",
    "                enc_out, tokens_bos, pad_idx=self.hparams.pad_index\n",
    "            )\n",
    "        # logits and softmax\n",
    "        pred = self.modules.seq_lin(dec_out)\n",
    "        p_seq = self.hparams.log_softmax(pred)\n",
    "\n",
    "        return (\n",
    "            p_seq\n",
    "        )\n",
    "\n",
    "    def compute_objectives(self, p_seq, batch, stage):\n",
    "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions : torch.Tensor\n",
    "            The model generated spectrograms and other metrics from `compute_forward`.\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            A one-element tensor used for backpropagating the gradient.\n",
    "        \"\"\"\n",
    "        tokens_eos, tokens_eos_lens = batch.code_eos\n",
    "\n",
    "        # speech translation loss\n",
    "        loss = self.hparams.seq_cost(p_seq, tokens_eos, length=tokens_eos_lens)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def init_optimizers(self):\n",
    "        \"\"\"Called during ``on_fit_start()``, initialize optimizers\n",
    "        after parameters are fully configured (e.g. DDP, jit).\n",
    "        \"\"\"\n",
    "        self.optimizers_dict = {}\n",
    "\n",
    "        # Initializes the wav2vec2 optimizer if the model is not wav2vec2_frozen\n",
    "        if not self.hparams.wav2vec2_frozen:\n",
    "            self.wav2vec_optimizer = self.hparams.wav2vec_opt_class(\n",
    "                self.modules.wav2vec2.parameters()\n",
    "            )\n",
    "            self.optimizers_dict[\"wav2vec_optimizer\"] = self.wav2vec_optimizer\n",
    "\n",
    "        self.model_optimizer = self.hparams.opt_class(\n",
    "            self.hparams.model.parameters()\n",
    "        )\n",
    "        self.optimizers_dict[\"model_optimizer\"] = self.model_optimizer\n",
    "\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.add_recoverable(\n",
    "                \"wav2vec_optimizer\", self.wav2vec_optimizer\n",
    "            )\n",
    "            self.checkpointer.add_recoverable(\n",
    "                \"model_optimizer\", self.model_optimizer\n",
    "            )\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"\"\"Gets called when a stage starts.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : Stage\n",
    "            The stage of the experiment: Stage.TRAIN, Stage.VALID, Stage.TEST\n",
    "        epoch : int\n",
    "            The current epoch count.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "\n",
    "            self.acc_metric = self.hparams.acc_computer()\n",
    "            self.bleu_metric = self.hparams.bleu_computer()\n",
    "            self.last_batch = None\n",
    "\n",
    "            logger.info(\"Loading pretrained HiFi-GAN ...\")\n",
    "            self.test_vocoder = UnitHIFIGAN.from_hparams(\n",
    "                source=self.hparams.vocoder_source,\n",
    "                savedir=self.hparams.vocoder_download_path,\n",
    "                run_opts={\"device\": \"cpu\"},\n",
    "            )\n",
    "\n",
    "            logger.info(\"Loading pretrained ASR ...\")\n",
    "            self.test_asr = EncoderDecoderASR.from_hparams(\n",
    "                source=self.hparams.asr_source,\n",
    "                savedir=self.hparams.asr_download_path,\n",
    "                run_opts={\"device\": \"cpu\"},\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Loading pretrained MEL-HIFI-GAN ...\")\n",
    "            self.mel_vocoder = HIFIGAN.from_hparams(\n",
    "                source=self.hparams.mel_hifigan_source,\n",
    "                savedir=self.hparams.mel_hifigan_download_path,\n",
    "                run_opts={\"device\": \"cpu\"},\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Loading pretrained speaker_adapter ...\")\n",
    "            self.speaker_adapter = self.hparams.speaker_adapter.eval()\n",
    "            self.speaker_adapter.load_state_dict(torch.load(self.hparams.speaker_adapter_source))\n",
    "            \n",
    "            logger.info(\"Loading pretrained var_predictor ...\")\n",
    "            self.var_predictor = self.hparams.var_predictor.eval()\n",
    "            self.var_predictor.load_state_dict(torch.load(self.hparams.var_predictor_source))\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-es-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/s2ut/888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usrs/zhou/.pyenv/versions/anaconda3-2020.02/envs/zhou_conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "hparams_file = \"hparams/inference.yaml\"\n",
    "with open(hparams_file) as fin:\n",
    "        hparams = load_hyperpyyaml(fin)\n",
    "\n",
    "    # If distributed_launch=True then\n",
    "    # create ddp_group with the right communication protocol\n",
    "\n",
    "    # Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "        experiment_directory=hparams[\"output_folder\"],\n",
    "        hyperparams_to_save=hparams_file,\n",
    "    )\n",
    "\n",
    "datasets = dataio_prepare(hparams)\n",
    "\n",
    "s2ut_brain = S2UT(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"opt_class\"],\n",
    "    hparams=hparams,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "s2ut_brain.on_evaluate_start(max_key=\"BLEU\")\n",
    "s2ut_brain.on_stage_start(Stage.TEST, epoch=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = datasets[\"test\"]\n",
    "test_dataloader_opts = {\n",
    "        \"batch_size\": 1,\n",
    "    }\n",
    "test_loader_kwargs=test_dataloader_opts\n",
    "if not (\n",
    "    isinstance(test_set, DataLoader)\n",
    "    or isinstance(test_set, LoopedLoader)\n",
    "):\n",
    "    test_loader_kwargs[\"ckpt_prefix\"] = None\n",
    "    test_set = s2ut_brain.make_dataloader(\n",
    "        test_set, Stage.TEST, **test_loader_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2ut_brain.modules.eval()\n",
    "cvsst_wavs = []\n",
    "s2ut_wavs = []\n",
    "src_wavs = []\n",
    "id_list = []\n",
    "scs2ut_wavs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, batch in enumerate(\n",
    "        test_set,\n",
    "    ):\n",
    "        s2ut_brain.step += 1\n",
    "        \n",
    "        batch = batch.to(s2ut_brain.device)\n",
    "        wavs, wav_lens = batch.src_sig\n",
    "        tgt_wav, tgt_wav_lens = batch.tgt_sig\n",
    "        src_wav, src_wav_lens = batch.src_sig\n",
    "        tokens_bos, _ = batch.code_bos\n",
    "        src_mel, src_mel_lens = batch.src_mel\n",
    "        src_mel[src_mel == s2ut_brain.hparams.pad_index] = 0\n",
    "        src_enc_out = s2ut_brain.speaker_adapter[0](src_mel.cpu())\n",
    "        # Use default padding value for wav2vec2\n",
    "        wavs[wavs == s2ut_brain.hparams.pad_index] = 0.0\n",
    "        print(\"-----\")\n",
    "        # compute features\n",
    "        enc_out = s2ut_brain.modules.wav2vec2(wavs, wav_lens)\n",
    "\n",
    "        # dimensionality reduction\n",
    "        enc_out = s2ut_brain.modules.enc(enc_out)\n",
    "\n",
    "        if isinstance(s2ut_brain.modules.transformer, DistributedDataParallel):\n",
    "            dec_out = s2ut_brain.modules.transformer.module.forward_mt_decoder_only(\n",
    "                enc_out, tokens_bos, pad_idx=s2ut_brain.hparams.pad_index\n",
    "            )\n",
    "        else:\n",
    "            dec_out = s2ut_brain.modules.transformer.forward_mt_decoder_only(\n",
    "                enc_out, tokens_bos, pad_idx=s2ut_brain.hparams.pad_index\n",
    "            )\n",
    "        \n",
    "        # logits and softmax\n",
    "\n",
    "        pred = s2ut_brain.modules.seq_lin(dec_out)\n",
    "\n",
    "        p_seq = s2ut_brain.hparams.log_softmax(pred)\n",
    "        hyps = None\n",
    "        wavs = None\n",
    "        transcripts = None\n",
    "        scs2ut_wavs = None\n",
    "        \n",
    "        ids = batch.id\n",
    "        tgt_text = batch.tgt_text\n",
    "\n",
    "        search = (\n",
    "            s2ut_brain.hparams.valid_search\n",
    "            if Stage.TEST == sb.Stage.VALID\n",
    "            else s2ut_brain.hparams.test_search\n",
    "            )\n",
    "        hyps, _, _, _ = search(enc_out.detach(), wav_lens)\n",
    "\n",
    "        # generate s2ut waveform\n",
    "        for hyp in hyps:\n",
    "            hyp = [x for x in hyp if x not in {100, 101, 102}]\n",
    "            if len(hyp) > 3:\n",
    "                \n",
    "                code = torch.LongTensor(hyp)\n",
    "                wav = s2ut_brain.test_vocoder.decode_unit(code)\n",
    "\n",
    "        # generete scs2ut waveform\n",
    "        for hyp in hyps:\n",
    "            hyp = [x for x in hyp if x not in {100, 101, 102}]\n",
    "            if len(hyp) > 3:\n",
    "                \n",
    "                code = torch.LongTensor(hyp)\n",
    "                \n",
    "                code = s2ut_brain.var_predictor(code)\n",
    "                fft_out = s2ut_brain.speaker_adapter[1](\n",
    "                    code, \n",
    "                    pad_idx=s2ut_brain.hparams.pad_index\n",
    "                )\n",
    "\n",
    "                pred_mel = s2ut_brain.speaker_adapter[2](src_enc_out, fft_out)\n",
    "                pred_mel = pred_mel.transpose(1,2)\n",
    "                scs2ut_wav = s2ut_brain.mel_vocoder.decode_batch(pred_mel).squeeze(1)\n",
    "\n",
    "        id_list.append(ids[0])\n",
    "        tgt_wavs.append(tgt_wav)\n",
    "        src_wavs.append(src_wav)\n",
    "        s2ut_wavs.append(wav)\n",
    "        scs2ut_wavs_list.append(scs2ut_wav)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tgt_wavs)):\n",
    "    print(\"----\")\n",
    "    print(\"src_audio: \")\n",
    "    display(Audio(data=src_wavs[i], rate=16000))\n",
    "    print(\"cvsst_audio: \")\n",
    "    display(Audio(data=tgt_wavs[i], rate=16000))\n",
    "    print(\"s2ut_audio: \")\n",
    "    display(Audio(data=s2ut_wavs[i], rate=16000))\n",
    "    print(\"scs2ut_audio: \")\n",
    "    display(Audio(data=scs2ut_wavs_list[i], rate=16000))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhou_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
